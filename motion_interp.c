// motion_interp.c
// Host-side Vulkan driver for the GPU motion interpolation pipeline.
// Manages pipeline creation, descriptor sets, buffer allocation,
// and dispatches the compute shader sequence.
//
// Compile with: -DUSE_VK  (Vulkan backend, default)
// The Vulkan types are forward-declared as void* in the header to avoid
// pulling in vulkan.h for users who only need the opaque API.
// Internally we include vulkan.h here.

#include "motion_interp.h"
#include <vulkan/vulkan.h>
#include <stdlib.h>
#include <string.h>
#include <stdio.h>
#include <math.h>
#include <assert.h>

// ──────────────────────────────────────────────────────────────────────────────
// Compiled SPIR-V blobs (generated by glslc at build time)
// In a real build these would be #include'd binary arrays or loaded from disk.
// We declare them as extern symbols here; the build system links them in.
// ──────────────────────────────────────────────────────────────────────────────
extern const uint32_t mi_pyramid_spv[];       extern const size_t mi_pyramid_spv_size;
extern const uint32_t mi_mv_upsample_spv[];   extern const size_t mi_mv_upsample_spv_size;
extern const uint32_t mi_analyze_spv[];       extern const size_t mi_analyze_spv_size;
extern const uint32_t mi_occlusion_spv[];     extern const size_t mi_occlusion_spv_size;
extern const uint32_t mi_flowfps_spv[];       extern const size_t mi_flowfps_spv_size;

// ──────────────────────────────────────────────────────────────────────────────
// Internal context
// ──────────────────────────────────────────────────────────────────────────────

#define MAX_PYRAMID_LEVELS 5
#define MV_COMPONENTS 4   // ivec4 per block

typedef struct GpuBuffer {
    VkBuffer       buffer;
    VkDeviceMemory memory;
    VkDeviceSize   size;
} GpuBuffer;

typedef struct GpuImage {
    VkImage        image;
    VkImageView    view;
    VkDeviceMemory memory;
    uint32_t       width, height;
} GpuImage;

struct MI_Context {
    VkDevice     device;
    MI_Config    cfg;
    MI_Pipeline  pipeline;

    // Derived dimensions
    int blocksX, blocksY, blockCount;

    // Pyramid luma images (current + reference)
    GpuImage pyramidCur[MAX_PYRAMID_LEVELS];
    GpuImage pyramidRef[MAX_PYRAMID_LEVELS];

    // Motion vector buffers (forward + backward), one per pyramid level
    GpuBuffer mvFwd[MAX_PYRAMID_LEVELS];
    GpuBuffer mvBwd[MAX_PYRAMID_LEVELS];

    // Accumulation buffers for flow splat (rgba32f)
    GpuImage accumFwd;
    GpuImage accumBwd;

    // Per-pixel mask image (rgba8)
    GpuImage mask;

    // Staging buffer for MV readback
    GpuBuffer staging;

    // Descriptor pool & sets (simplified: one pool, sets allocated per pipeline)
    VkDescriptorPool descPool;

    // Sampler
    VkSampler linearSampler;
};

// ──────────────────────────────────────────────────────────────────────────────
// Utilities
// ──────────────────────────────────────────────────────────────────────────────

static uint32_t findMemoryType(VkPhysicalDevice physDev,
                                uint32_t typeBits,
                                VkMemoryPropertyFlags props) {
    VkPhysicalDeviceMemoryProperties memProps;
    vkGetPhysicalDeviceMemoryProperties(physDev, &memProps);
    for (uint32_t i = 0; i < memProps.memoryTypeCount; i++) {
        if ((typeBits & (1u << i)) &&
            (memProps.memoryTypes[i].propertyFlags & props) == props)
            return i;
    }
    fprintf(stderr, "[MI] No suitable memory type found\n");
    return 0;
}

static VkResult createBuffer(MI_Context* ctx, VkPhysicalDevice physDev,
                              VkDeviceSize size, VkBufferUsageFlags usage,
                              VkMemoryPropertyFlags memFlags, GpuBuffer* out) {
    VkBufferCreateInfo bci = {
        .sType = VK_STRUCTURE_TYPE_BUFFER_CREATE_INFO,
        .size  = size, .usage = usage,
        .sharingMode = VK_SHARING_MODE_EXCLUSIVE
    };
    VkResult r = vkCreateBuffer(ctx->device, &bci, NULL, &out->buffer);
    if (r != VK_SUCCESS) return r;

    VkMemoryRequirements req;
    vkGetBufferMemoryRequirements(ctx->device, out->buffer, &req);

    VkMemoryAllocateInfo ai = {
        .sType           = VK_STRUCTURE_TYPE_MEMORY_ALLOCATE_INFO,
        .allocationSize  = req.size,
        .memoryTypeIndex = findMemoryType(physDev, req.memoryTypeBits, memFlags)
    };
    r = vkAllocateMemory(ctx->device, &ai, NULL, &out->memory);
    if (r != VK_SUCCESS) { vkDestroyBuffer(ctx->device, out->buffer, NULL); return r; }
    vkBindBufferMemory(ctx->device, out->buffer, out->memory, 0);
    out->size = size;
    return VK_SUCCESS;
}

static VkShaderModule createShaderModule(VkDevice dev,
                                          const uint32_t* code, size_t size) {
    VkShaderModuleCreateInfo ci = {
        .sType    = VK_STRUCTURE_TYPE_SHADER_MODULE_CREATE_INFO,
        .codeSize = size,
        .pCode    = code
    };
    VkShaderModule mod;
    if (vkCreateShaderModule(dev, &ci, NULL, &mod) != VK_SUCCESS) return VK_NULL_HANDLE;
    return mod;
}

// Create a compute pipeline with specialization constants for block config
static VkPipeline createComputePipeline(VkDevice dev,
                                         VkPipelineLayout layout,
                                         VkShaderModule module,
                                         const VkSpecializationInfo* specInfo) {
    VkComputePipelineCreateInfo ci = {
        .sType  = VK_STRUCTURE_TYPE_COMPUTE_PIPELINE_CREATE_INFO,
        .layout = layout,
        .stage  = {
            .sType  = VK_STRUCTURE_TYPE_PIPELINE_SHADER_STAGE_CREATE_INFO,
            .stage  = VK_SHADER_STAGE_COMPUTE_BIT,
            .module = module,
            .pName  = "main",
            .pSpecializationInfo = specInfo
        }
    };
    VkPipeline pip;
    if (vkCreateComputePipelines(dev, VK_NULL_HANDLE, 1, &ci, NULL, &pip) != VK_SUCCESS)
        return VK_NULL_HANDLE;
    return pip;
}

// ──────────────────────────────────────────────────────────────────────────────
// Pipeline layout (shared across all compute shaders via push constants)
// ──────────────────────────────────────────────────────────────────────────────

// Max push constant size used: 56 bytes (safe for all Vulkan implementations)
#define PUSH_CONST_SIZE 64

static VkPipelineLayout createSharedPipelineLayout(VkDevice dev,
                                                     VkDescriptorSetLayout* dsLayouts,
                                                     uint32_t layoutCount) {
    VkPushConstantRange pcRange = {
        .stageFlags = VK_SHADER_STAGE_COMPUTE_BIT,
        .offset     = 0,
        .size       = PUSH_CONST_SIZE
    };
    VkPipelineLayoutCreateInfo ci = {
        .sType                  = VK_STRUCTURE_TYPE_PIPELINE_LAYOUT_CREATE_INFO,
        .setLayoutCount         = layoutCount,
        .pSetLayouts            = dsLayouts,
        .pushConstantRangeCount = 1,
        .pPushConstantRanges    = &pcRange
    };
    VkPipelineLayout layout;
    vkCreatePipelineLayout(dev, &ci, NULL, &layout);
    return layout;
}

// ──────────────────────────────────────────────────────────────────────────────
// MI_Create
// ──────────────────────────────────────────────────────────────────────────────

MI_Context* MI_Create(void* vkDevice, void* allocator, const MI_Config* cfg) {
    (void)allocator; // using simple vkAllocateMemory for now
    VkDevice dev = (VkDevice)vkDevice;

    MI_Context* ctx = calloc(1, sizeof(MI_Context));
    if (!ctx) return NULL;
    ctx->device = dev;
    ctx->cfg    = *cfg;

    // Derived block grid
    int step         = cfg->blockSize - cfg->blockOverlap;
    ctx->blocksX     = (cfg->frameWidth  + step - 1) / step;
    ctx->blocksY     = (cfg->frameHeight + step - 1) / step;
    ctx->blockCount  = ctx->blocksX * ctx->blocksY;

    // ── Build specialization constants for motion_analyze ────────────────────
    // Spec constant IDs match layout(constant_id = N) in the GLSL shaders
    struct {
        int   blockSize;
        int   searchRange;
        int   searchThreads;
        int   useSATD;    // VkBool32 as int
        int   subpel;
        int   overlap;
    } analyzeSpec = {
        .blockSize     = cfg->blockSize,
        .searchRange   = cfg->searchRange,
        .searchThreads = 64,
        .useSATD       = cfg->useSATD ? 1 : 0,
        .subpel        = cfg->subpelRefine ? 1 : 0,
        .overlap       = cfg->blockOverlap
    };

    VkSpecializationMapEntry analyzeEntries[6] = {
        {0, offsetof(typeof(analyzeSpec), blockSize),     sizeof(int)},
        {1, offsetof(typeof(analyzeSpec), searchRange),   sizeof(int)},
        {2, offsetof(typeof(analyzeSpec), searchThreads), sizeof(int)},
        {3, offsetof(typeof(analyzeSpec), useSATD),       sizeof(int)},
        {4, offsetof(typeof(analyzeSpec), subpel),        sizeof(int)},
        {5, offsetof(typeof(analyzeSpec), overlap),       sizeof(int)},
    };
    VkSpecializationInfo analyzeSpecInfo = {
        .mapEntryCount = 6,
        .pMapEntries   = analyzeEntries,
        .dataSize      = sizeof(analyzeSpec),
        .pData         = &analyzeSpec
    };

    // ── Create shader modules ────────────────────────────────────────────────
    VkShaderModule mPyramid   = createShaderModule(dev, mi_pyramid_spv,     mi_pyramid_spv_size);
    VkShaderModule mMvUp      = createShaderModule(dev, mi_mv_upsample_spv, mi_mv_upsample_spv_size);
    VkShaderModule mAnalyze   = createShaderModule(dev, mi_analyze_spv,     mi_analyze_spv_size);
    VkShaderModule mOcclusion = createShaderModule(dev, mi_occlusion_spv,   mi_occlusion_spv_size);
    VkShaderModule mFlow      = createShaderModule(dev, mi_flowfps_spv,     mi_flowfps_spv_size);

    // Pipeline layout (single set layout — simplified; a full implementation
    // would have one descriptor set layout per pipeline stage)
    // For brevity, the layout creation is sketched here:
    // TODO: create VkDescriptorSetLayout binding structs and call
    //       vkCreateDescriptorSetLayout for each pipeline stage
    VkPipelineLayout layout = createSharedPipelineLayout(dev, NULL, 0);
    ctx->pipeline.pipelineLayout = (void*)layout;

    // ── Create compute pipelines ─────────────────────────────────────────────
    int passA = 0, passB = 1;
    struct { int blockSize; int overlap; int pass; int mask; int blend; } flowSpec;

    // Pass A (splat)
    flowSpec = (typeof(flowSpec)){cfg->blockSize, cfg->blockOverlap, 0, 1, 1};
    VkSpecializationMapEntry flowEntries[5] = {
        {0, 0,  sizeof(int)},
        {1, 4,  sizeof(int)},
        {2, 8,  sizeof(int)},
        {3, 12, sizeof(int)},
        {4, 16, sizeof(int)}
    };
    VkSpecializationInfo flowSpecA = {5, flowEntries, sizeof(flowSpec), &flowSpec};
    ctx->pipeline.flowFpsSplatPipeline = (void*)createComputePipeline(dev, layout, mFlow, &flowSpecA);

    // Pass B (blend)
    flowSpec.pass = 1;
    VkSpecializationInfo flowSpecB = {5, flowEntries, sizeof(flowSpec), &flowSpec};
    ctx->pipeline.flowFpsBlendPipeline = (void*)createComputePipeline(dev, layout, mFlow, &flowSpecB);

    ctx->pipeline.analyzeComputePipeline = (void*)createComputePipeline(dev, layout, mAnalyze, &analyzeSpecInfo);
    ctx->pipeline.pyramidDownPipeline    = (void*)createComputePipeline(dev, layout, mPyramid, NULL);
    ctx->pipeline.mvUpscalePipeline      = (void*)createComputePipeline(dev, layout, mMvUp, NULL);
    ctx->pipeline.occlusionPipeline      = (void*)createComputePipeline(dev, layout, mOcclusion, NULL);

    // Free shader modules (no longer needed after pipeline creation)
    vkDestroyShaderModule(dev, mPyramid,   NULL);
    vkDestroyShaderModule(dev, mMvUp,      NULL);
    vkDestroyShaderModule(dev, mAnalyze,   NULL);
    vkDestroyShaderModule(dev, mOcclusion, NULL);
    vkDestroyShaderModule(dev, mFlow,      NULL);

    fprintf(stderr, "[MI] Context created: %dx%d, %dx%d blocks, %d levels\n",
            cfg->frameWidth, cfg->frameHeight,
            ctx->blocksX, ctx->blocksY,
            cfg->pyramidLevels);
    return ctx;
}

// ──────────────────────────────────────────────────────────────────────────────
// MI_Analyze
// ──────────────────────────────────────────────────────────────────────────────

void MI_Analyze(MI_Context* ctx, void* vkCmdBuf, void* frame0, void* frame1) {
    VkCommandBuffer cmd = (VkCommandBuffer)vkCmdBuf;

    // ── Step 1: Build image pyramids ─────────────────────────────────────────
    // For each level > 0, dispatch pyramid_downsample.glsl on previous level
    int levels = ctx->cfg.pyramidLevels;
    for (int lv = 1; lv < levels; lv++) {
        // Push constants: dstSize = frameSize >> lv
        struct { int32_t w, h; } pc = {
            ctx->cfg.frameWidth  >> lv,
            ctx->cfg.frameHeight >> lv
        };
        vkCmdBindPipeline(cmd, VK_PIPELINE_BIND_POINT_COMPUTE,
                          (VkPipeline)ctx->pipeline.pyramidDownPipeline);
        vkCmdPushConstants(cmd, (VkPipelineLayout)ctx->pipeline.pipelineLayout,
                           VK_SHADER_STAGE_COMPUTE_BIT, 0, sizeof(pc), &pc);
        // Dispatch: one thread per output pixel, 8x8 workgroups
        uint32_t gx = (pc.w  + 7) / 8;
        uint32_t gy = (pc.h  + 7) / 8;
        // TODO: bind descriptor sets for level lv-1 → level lv
        vkCmdDispatch(cmd, gx, gy, 1);
        // Memory barrier between pyramid levels
        VkMemoryBarrier mb = { VK_STRUCTURE_TYPE_MEMORY_BARRIER,
                               NULL,
                               VK_ACCESS_SHADER_WRITE_BIT,
                               VK_ACCESS_SHADER_READ_BIT };
        vkCmdPipelineBarrier(cmd,
            VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT,
            VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT,
            0, 1, &mb, 0, NULL, 0, NULL);
    }

    // ── Step 2: Coarse-to-fine motion estimation ─────────────────────────────
    int step = ctx->cfg.blockSize - ctx->cfg.blockOverlap;

    for (int lv = levels - 1; lv >= 0; lv--) {
        int scale    = 1 << lv;
        int bx       = (ctx->cfg.frameWidth  / scale + step - 1) / step;
        int by       = (ctx->cfg.frameHeight / scale + step - 1) / step;

        // Push constants for motion_analyze
        struct {
            int32_t fwX, fwY;   // frameSize
            int32_t bkX, bkY;   // blocksSize
            int32_t blocksX;
            int32_t level;
            int32_t levelScale;
            float   lambda;
            int32_t predCount;
        } pc = {
            ctx->cfg.frameWidth  / scale,
            ctx->cfg.frameHeight / scale,
            bx, by, bx, lv, scale,
            ctx->cfg.lambda, 1
        };

        vkCmdBindPipeline(cmd, VK_PIPELINE_BIND_POINT_COMPUTE,
                          (VkPipeline)ctx->pipeline.analyzeComputePipeline);
        // TODO: bind descriptor sets for level lv (pyramid images + MV buffers)
        vkCmdPushConstants(cmd, (VkPipelineLayout)ctx->pipeline.pipelineLayout,
                           VK_SHADER_STAGE_COMPUTE_BIT, 0, sizeof(pc), &pc);
        // One workgroup per block
        vkCmdDispatch(cmd, (uint32_t)bx, (uint32_t)by, 1);

        VkMemoryBarrier mb = { VK_STRUCTURE_TYPE_MEMORY_BARRIER, NULL,
                               VK_ACCESS_SHADER_WRITE_BIT,
                               VK_ACCESS_SHADER_READ_BIT };
        vkCmdPipelineBarrier(cmd,
            VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT,
            VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT,
            0, 1, &mb, 0, NULL, 0, NULL);

        // If not at full-res yet, upsample MVs to the next finer level
        if (lv > 0) {
            // TODO: bind upsample descriptors (coarse MV → fine MV predictor)
            vkCmdBindPipeline(cmd, VK_PIPELINE_BIND_POINT_COMPUTE,
                              (VkPipeline)ctx->pipeline.mvUpscalePipeline);
            vkCmdDispatch(cmd, (uint32_t)bx * 2, (uint32_t)by * 2, 1);
            vkCmdPipelineBarrier(cmd,
                VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT,
                VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT,
                0, 1, &mb, 0, NULL, 0, NULL);
        }
    }

    // ── Step 3: Occlusion detection ──────────────────────────────────────────
    struct {
        int32_t fwX, fwY;
        int32_t bkX, bkY;
        int32_t blocksX;
        int32_t blockSize;
        int32_t blockStep;
        float   fbConsistThresh;
        float   sadThresh;
    } occPC = {
        ctx->cfg.frameWidth, ctx->cfg.frameHeight,
        ctx->blocksX, ctx->blocksY, ctx->blocksX,
        ctx->cfg.blockSize, step,
        ctx->cfg.fbConsistThresh,
        ctx->cfg.sadThreshold
    };

    vkCmdBindPipeline(cmd, VK_PIPELINE_BIND_POINT_COMPUTE,
                      (VkPipeline)ctx->pipeline.occlusionPipeline);
    vkCmdPushConstants(cmd, (VkPipelineLayout)ctx->pipeline.pipelineLayout,
                       VK_SHADER_STAGE_COMPUTE_BIT, 0, sizeof(occPC), &occPC);
    uint32_t gx = ((uint32_t)ctx->cfg.frameWidth  + 7) / 8;
    uint32_t gy = ((uint32_t)ctx->cfg.frameHeight + 7) / 8;
    vkCmdDispatch(cmd, gx, gy, 1);

    VkMemoryBarrier mb2 = { VK_STRUCTURE_TYPE_MEMORY_BARRIER, NULL,
                            VK_ACCESS_SHADER_WRITE_BIT,
                            VK_ACCESS_SHADER_READ_BIT };
    vkCmdPipelineBarrier(cmd,
        VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT,
        VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT,
        0, 1, &mb2, 0, NULL, 0, NULL);
}

// ──────────────────────────────────────────────────────────────────────────────
// MI_Interpolate
// ──────────────────────────────────────────────────────────────────────────────

void MI_Interpolate(MI_Context* ctx, void* vkCmdBuf,
                    void* frame0, void* frame1, void* output, float t) {
    VkCommandBuffer cmd = (VkCommandBuffer)vkCmdBuf;
    int step = ctx->cfg.blockSize - ctx->cfg.blockOverlap;

    // ── Pass A: Clear accumulation buffers and splat pixels ──────────────────
    // (accumFwd and accumBwd cleared via renderpass loadOp or explicit fill)
    VkClearColorValue zero = {.float32 = {0,0,0,0}};
    VkImageSubresourceRange range = {VK_IMAGE_ASPECT_COLOR_BIT, 0,1,0,1};
    // TODO: transition accumFwd/Bwd to TRANSFER_DST, clear, then to GENERAL
    vkCmdClearColorImage(cmd,
        ctx->accumFwd.image, VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL,
        &zero, 1, &range);
    vkCmdClearColorImage(cmd,
        ctx->accumBwd.image, VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL,
        &zero, 1, &range);

    struct {
        int32_t fwX, fwY;
        int32_t bkX, bkY;
        int32_t blocksX;
        float   t;
        float   sadThresh;
        float   maskBlend;
    } splPC = {
        ctx->cfg.frameWidth, ctx->cfg.frameHeight,
        ctx->blocksX, ctx->blocksY, ctx->blocksX,
        t, ctx->cfg.sadThreshold, 0.5f
    };

    vkCmdBindPipeline(cmd, VK_PIPELINE_BIND_POINT_COMPUTE,
                      (VkPipeline)ctx->pipeline.flowFpsSplatPipeline);
    // TODO: bind descriptors (frame0, frame1, mvFwd, mvBwd, accumFwd, accumBwd, output, mask)
    vkCmdPushConstants(cmd, (VkPipelineLayout)ctx->pipeline.pipelineLayout,
                       VK_SHADER_STAGE_COMPUTE_BIT, 0, sizeof(splPC), &splPC);
    // One workgroup per block, block-sized local groups
    vkCmdDispatch(cmd, (uint32_t)ctx->blocksX, (uint32_t)ctx->blocksY, 1);

    VkMemoryBarrier mb = { VK_STRUCTURE_TYPE_MEMORY_BARRIER, NULL,
                           VK_ACCESS_SHADER_WRITE_BIT,
                           VK_ACCESS_SHADER_READ_BIT };
    vkCmdPipelineBarrier(cmd,
        VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT,
        VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT,
        0, 1, &mb, 0, NULL, 0, NULL);

    // ── Pass B: Normalize + blend ────────────────────────────────────────────
    vkCmdBindPipeline(cmd, VK_PIPELINE_BIND_POINT_COMPUTE,
                      (VkPipeline)ctx->pipeline.flowFpsBlendPipeline);
    vkCmdPushConstants(cmd, (VkPipelineLayout)ctx->pipeline.pipelineLayout,
                       VK_SHADER_STAGE_COMPUTE_BIT, 0, sizeof(splPC), &splPC);
    uint32_t gx = ((uint32_t)ctx->cfg.frameWidth  + (uint32_t)ctx->cfg.blockSize - 1) / (uint32_t)ctx->cfg.blockSize;
    uint32_t gy = ((uint32_t)ctx->cfg.frameHeight + (uint32_t)ctx->cfg.blockSize - 1) / (uint32_t)ctx->cfg.blockSize;
    vkCmdDispatch(cmd, gx, gy, 1);
}

// ──────────────────────────────────────────────────────────────────────────────
// MI_Destroy
// ──────────────────────────────────────────────────────────────────────────────

void MI_Destroy(MI_Context* ctx) {
    if (!ctx) return;
    VkDevice dev = ctx->device;
    VkPipelineLayout layout = (VkPipelineLayout)ctx->pipeline.pipelineLayout;

#define DESTROY_PIPE(p) if (p) vkDestroyPipeline(dev, (VkPipeline)(p), NULL)
    DESTROY_PIPE(ctx->pipeline.pyramidDownPipeline);
    DESTROY_PIPE(ctx->pipeline.mvUpscalePipeline);
    DESTROY_PIPE(ctx->pipeline.analyzeComputePipeline);
    DESTROY_PIPE(ctx->pipeline.occlusionPipeline);
    DESTROY_PIPE(ctx->pipeline.flowFpsSplatPipeline);
    DESTROY_PIPE(ctx->pipeline.flowFpsBlendPipeline);
#undef DESTROY_PIPE
    if (layout) vkDestroyPipelineLayout(dev, layout, NULL);
    // TODO: destroy buffers, images, sampler, descriptor pool
    free(ctx);
}

void MI_GetBlockGrid(const MI_Context* ctx, int* outBlocksX, int* outBlocksY) {
    if (outBlocksX) *outBlocksX = ctx->blocksX;
    if (outBlocksY) *outBlocksY = ctx->blocksY;
}

const MI_Pipeline* MI_GetPipeline(const MI_Context* ctx) {
    return &ctx->pipeline;
}
